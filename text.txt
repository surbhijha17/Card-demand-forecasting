def calculate_wer(reference, generated):
    # Tokenize the reference and generated transcripts into words
    ref_tokens = reference.lower().split()
    gen_tokens = generated.lower().split()

    # Calculate Levenshtein distance using dynamic programming
    dp = [[0] * (len(gen_tokens) + 1) for _ in range(len(ref_tokens) + 1)]
    for i in range(len(ref_tokens) + 1):
        dp[i][0] = i
    for j in range(len(gen_tokens) + 1):
        dp[0][j] = j
    for i in range(1, len(ref_tokens) + 1):
        for j in range(1, len(gen_tokens) + 1):
            if ref_tokens[i - 1] == gen_tokens[j - 1]:
                dp[i][j] = dp[i - 1][j - 1]
            else:
                dp[i][j] = 1 + min(dp[i - 1][j], dp[i][j - 1], dp[i - 1][j - 1])

    # Backtrace to classify errors (insertions, deletions, substitutions)
    insertions, deletions, substitutions = 0, 0, 0
    i, j = len(ref_tokens), len(gen_tokens)
    while i > 0 and j > 0:
        if ref_tokens[i - 1] == gen_tokens[j - 1]:
            i -= 1
            j -= 1
        else:
            if dp[i][j] == dp[i - 1][j - 1] + 1:
                substitutions += 1
                i -= 1
                j -= 1
            elif dp[i][j] == dp[i][j - 1] + 1:
                insertions += 1
                j -= 1
            else:
                deletions += 1
                i -= 1

    # Handle any remaining tokens in the reference or generated transcript
    insertions += j
    deletions += i

    # Compute WER
    wer = (insertions + deletions + substitutions) / len(ref_tokens)
    return wer

# Example usage:
reference_transcript = "the quick brown fox jumps over the lazy dog"
generated_transcript = "the quick fox jumps the lazy dog"
wer = calculate_wer(reference_transcript, generated_transcript)
print("Word Error Rate (WER): {:.2%}".format(wer))
