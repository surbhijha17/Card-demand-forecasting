def calculate_wer_with_custom_weights(reference, generated, weight_critical=1.5, weight_non_critical=1.0):
    # Tokenize the reference and generated transcripts into words
    ref_tokens = reference.lower().split()
    gen_tokens = generated.lower().split()

    # Calculate Levenshtein distance using dynamic programming
    dp = [[0] * (len(gen_tokens) + 1) for _ in range(len(ref_tokens) + 1)]
    for i in range(len(ref_tokens) + 1):
        dp[i][0] = i
    for j in range(len(gen_tokens) + 1):
        dp[0][j] = j
    for i in range(1, len(ref_tokens) + 1):
        for j in range(1, len(gen_tokens) + 1):
            if ref_tokens[i - 1] == gen_tokens[j - 1]:
                dp[i][j] = dp[i - 1][j - 1]
            else:
                dp[i][j] = 1 + min(dp[i - 1][j], dp[i][j - 1], dp[i - 1][j - 1])

    # Backtrace to classify errors (insertions, deletions, substitutions)
    insertions, deletions, substitutions = 0, 0, 0
    i, j = len(ref_tokens), len(gen_tokens)
    while i > 0 and j > 0:
        if ref_tokens[i - 1] == gen_tokens[j - 1]:
            i -= 1
            j -= 1
        else:
            if dp[i][j] == dp[i - 1][j - 1] + 1:
                if gen_tokens[j - 1] in ref_tokens:
                    substitutions += 1
                else:
                    # Check if the inserted word is critical or non-critical
                    # You may have a custom function to determine this based on the word itself.
                    inserted_word = gen_tokens[j - 1]
                    if is_critical(inserted_word):
                        insertions += weight_critical
                    else:
                        insertions += weight_non_critical
                i -= 1
                j -= 1
            elif dp[i][j] == dp[i][j - 1] + 1:
                # The inserted word is always an insertion
                insertions += weight_non_critical  # You may choose a custom weight for non-critical insertions.
                j -= 1
            else:
                deletions += 1
                i -= 1

    # Handle any remaining tokens in the reference or generated transcript
    insertions += j
    deletions += i

    # Compute WER
    wer = (insertions + deletions + substitutions) / len(ref_tokens)
    return wer

# Example usage:
reference_transcript = "the quick brown fox jumps over the lazy dog"
generated_transcript = "the quick fox jumps the really lazy dog"
wer = calculate_wer_with_custom_weights(reference_transcript, generated_transcript)
print("Word Error Rate (WER): {:.2%}".format(wer))
