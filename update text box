def calculate_wer_with_custom_weights(reference, generated, weight_critical=1.5, weight_non_critical=1.0):
    # Tokenize the reference and generated transcripts into words
    ref_tokens = reference.lower().split()
    gen_tokens = generated.lower().split()

    # Calculate Levenshtein distance using dynamic programming
    dp = [[0] * (len(gen_tokens) + 1) for _ in range(len(ref_tokens) + 1)]
    for i in range(len(ref_tokens) + 1):
        dp[i][0] = i
    for j in range(len(gen_tokens) + 1):
        dp[0][j] = j
    for i in range(1, len(ref_tokens) + 1):
        for j in range(1, len(gen_tokens) + 1):
            if ref_tokens[i - 1] == gen_tokens[j - 1]:
                dp[i][j] = dp[i - 1][j - 1]
            else:
                dp[i][j] = 1 + min(dp[i - 1][j], dp[i][j - 1], dp[i - 1][j - 1])

    # Backtrace to classify errors (insertions, deletions, substitutions)
    insertions, deletions, substitutions = 0, 0, 0
    i, j = len(ref_tokens), len(gen_tokens)
    while i > 0 and j > 0:
        if ref_tokens[i - 1] == gen_tokens[j - 1]:
            i -= 1
            j -= 1
        else:
            if dp[i][j] == dp[i - 1][j - 1] + 1:
                if gen_tokens[j - 1] in ref_tokens:
                    # Substitution
                    substitutions += weight_critical if is_critical(gen_tokens[j - 1]) else weight_non_critical
                else:
                    # Insertion
                    insertions += weight_critical if is_critical(gen_tokens[j - 1]) else weight_non_critical
                i -= 1
                j -= 1
            elif dp[i][j] == dp[i][j - 1] + 1:
                # Insertion
                insertions += weight_critical if is_critical(gen_tokens[j - 1]) else weight_non_critical
                j -= 1
            else:
                # Deletion
                deletions += weight_critical if is_critical(ref_tokens[i - 1]) else weight_non_critical
                i -= 1

    # Handle any remaining tokens in the reference or generated transcript
    while i > 0:
        deletions += weight_critical if is_critical(ref_tokens[i - 1]) else weight_non_critical
        i -= 1
    while j > 0:
        insertions += weight_critical if is_critical(gen_tokens[j - 1]) else weight_non_critical
        j -= 1

    # Compute WER
    total_words = len(ref_tokens)
    wer = (insertions + deletions + substitutions) / total_words
    return wer

# Example usage:
reference_transcript = "the quick brown fox jumps over the lazy dog"
generated_transcript = "the quick fox jumps the really lazy dog"
wer = calculate_wer_with_custom_weights(reference_transcript, generated_transcript)
print("Word Error Rate (WER): {:.2%}".format(wer))
